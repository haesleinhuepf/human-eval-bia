{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66d7773-0e61-4f6a-9005-6422684826cf",
   "metadata": {},
   "source": [
    "# Creating test cases\n",
    "This notebook takes a folder of notebooks and turns them into a jsonl file in the format human_eval expects.\n",
    "\n",
    "It also verifies if all test cases are categorised in the categorise_functions yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72835d2a-3a74-4cfe-91a6-2349a342105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ff314f-fc13-4f95-987d-68803d568023",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_notebook_directory = '../test_cases/' # must end with /\n",
    "# Specify the filename to save the .jsonl file\n",
    "target_jsonl_filename = '../data/human-eval-bia.jsonl'\n",
    "categories_file = \"../data/human-eval-bia-categories.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c6c23d-464d-4540-a58c-09683e70bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_33544\\843600576.py:42: UserWarning: I had issues reading a cell in ../test_cases/fft_spectrum.ipynb starting with \n",
      "  warnings.warn(f\"I had issues reading a cell in {task_id} starting with \")\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_33544\\843600576.py:42: UserWarning: I had issues reading a cell in ../test_cases/workflow_watershed_segmentation_correction_measurement.ipynb starting with \n",
      "  warnings.warn(f\"I had issues reading a cell in {task_id} starting with \")\n"
     ]
    }
   ],
   "source": [
    "list_of_cases = []\n",
    "\n",
    "\n",
    "# List all files in the current directory\n",
    "files = os.listdir(source_notebook_directory)\n",
    "\n",
    "# Iterate through the files and print names ending with .ipynb\n",
    "for file in files:\n",
    "    if file.endswith('.ipynb'):\n",
    "        notebook_filename = source_notebook_directory + file\n",
    "        \n",
    "        # Load and parse the notebook\n",
    "        with open(notebook_filename, 'r') as file:\n",
    "            notebook = json.load(file)\n",
    "        \n",
    "        task_id = notebook_filename\n",
    "        prompt = None\n",
    "        canonical_solution = None\n",
    "        entry_point = None\n",
    "        test = None\n",
    "        \n",
    "        # Iterate through the cells and print the source of code cells\n",
    "        for cell in notebook['cells']:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                # Joining the lines of code for better readability\n",
    "                code = ''.join(cell['source'])\n",
    "                # print('\\n\\nCODE\\n\\n',code)\n",
    "        \n",
    "                if code.startswith('check('):\n",
    "                    entry_point = code.strip().replace(\"check(\",\"\").replace(\")\",\"\").strip()\n",
    "                elif '\"\"\"' in code:\n",
    "                    temp = code.split('\"\"\"')\n",
    "                    canonical_solution = temp[-1]\n",
    "                    temp[-1] = \"\"\n",
    "                    prompt = '\"\"\"'.join(temp)\n",
    "                elif 'def check(' in code:\n",
    "                    test = code \n",
    "                elif len(code.strip()) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    sample = code[:20]\n",
    "                    warnings.warn(f\"I had issues reading a cell in {task_id} starting with \")\n",
    "                    \n",
    "        if prompt is None:\n",
    "            warnings.warn(f\"Couldn't extract prompt from {task_id}.\")\n",
    "        elif canonical_solution is None:\n",
    "            warnings.warn(f\"Couldn't extract canonical_solution from {task_id}.\")\n",
    "        elif entry_point is None:\n",
    "            warnings.warn(f\"Couldn't extract entry_point from {task_id}.\")\n",
    "        \n",
    "        test_case = {\n",
    "            'task_id':task_id,\n",
    "            'prompt':prompt,\n",
    "            'canonical_solution':canonical_solution,\n",
    "            'entry_point':entry_point,\n",
    "            'test':test\n",
    "        }\n",
    "\n",
    "        #print(test_case)\n",
    "        list_of_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2baf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that the test cases are in the categorise_functions yaml file \n",
    "#Raise a Value Error if not\n",
    "\n",
    "import yaml \n",
    "\n",
    "# Read YAML file where each test case has a category\n",
    "with open(categories_file, 'r') as stream:\n",
    "    categorise_functions = yaml.safe_load(stream)\n",
    "\n",
    "all_categories = []\n",
    "\n",
    "#for each test_case in list_of_cases, check if they are in keys in categorise_functions\n",
    "for test_case in list_of_cases:\n",
    "    #get the name of the test case from filepath in task id\n",
    "    test_case_name = os.path.splitext(os.path.basename(test_case['task_id']))[0]\n",
    "    #verify if the test case is in the categorise_functions file\n",
    "    if test_case_name not in categorise_functions.keys():\n",
    "        print(f\"{test_case_name} not found in categorise_functions\")\n",
    "    else:\n",
    "        for c in categorise_functions[test_case_name]:\n",
    "            all_categories.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b026d94-ae61-4c32-be99-cd2695fbf9e7",
   "metadata": {},
   "source": [
    "Here we can check if some appear multiple times, e.g. with typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5850d2d2-268d-4634-b4b7-c9db1816fa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_wrangling',\n",
       " 'feature_extraction',\n",
       " 'file_i_o',\n",
       " 'hello_world',\n",
       " 'image_filtering',\n",
       " 'image_preprocessing',\n",
       " 'image_transformation',\n",
       " 'measurement',\n",
       " 'morphological_operations',\n",
       " 'segmentation',\n",
       " 'segmentation_post_processing',\n",
       " 'statistical_analysis',\n",
       " 'workflow_automation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(all_categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339ee8e9-457e-4d4f-a521-f758c36fb3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to ../data/human-eval-bia.jsonl.\n"
     ]
    }
   ],
   "source": [
    "# Open the file in write mode and save the dictionaries\n",
    "with open(target_jsonl_filename, 'w') as file:\n",
    "    for dictionary in list_of_cases:\n",
    "        # Convert dictionary to a JSON formatted string and write it\n",
    "        json_str = json.dumps(dictionary)\n",
    "        file.write(json_str + '\\n')\n",
    "\n",
    "print(f'Data successfully saved to {target_jsonl_filename}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d31eec-6e3b-42ff-8638-e172c84827f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_otsu_threshold_and_count_positive_pixels\n",
      "binary_closing\n",
      "binary_skeleton\n",
      "bland_altman\n",
      "combine_columns_of_tables\n",
      "create_polygon_from_coordinates\n",
      "convex_hull_measure_area\n",
      "convolve_images\n",
      "count_number_of_touching_neighbors\n",
      "count_objects_over_time\n",
      "count_overlapping_regions\n",
      "create_umap\n",
      "crop_quarter_image\n",
      "dataframe_colum_rename\n",
      "deconvolve_image\n",
      "detect_edges\n",
      "detect_ellipse\n",
      "distance_between_maxima\n",
      "expand_labels_without_overlap\n",
      "extract_surface_measure_area\n",
      "fft_spectrum\n",
      "find_closest_neighbors\n",
      "fit_circle\n",
      "fit_gaussian_to_spot\n",
      "flow_field_deformation\n",
      "generate_image_histogram\n",
      "identify_centroids\n",
      "interpolate_stack\n",
      "label_binary_image_and_count_labels\n",
      "label_sequentially\n",
      "linear_intensity_profile\n",
      "list_image_files_in_folder\n",
      "load_tif_and_output_rgb\n",
      "local_maxima_from_distance_transform\n",
      "map_pixel_count_of_labels\n",
      "mask_image\n",
      "maximum_intensity_projection\n",
      "mean_squared_error\n",
      "mean_std_column\n",
      "measure_aspect_ratio_of_regions\n",
      "measure_intensity_of_labels\n",
      "measure_intensity_over_time\n",
      "measure_mean_image_intensity\n",
      "measure_pixel_count_of_labels\n",
      "measure_properties_of_regions\n",
      "open_image_read_voxel_size\n",
      "open_image_return_dimensions\n",
      "open_nifti_image\n",
      "open_zarr\n",
      "pair_wise_correlation_matrix\n",
      "radial_intensity_profile\n",
      "read_imagej_tif_metadata\n",
      "read_ome_metadata_from_ome_xml\n",
      "region_growing_segmentation\n",
      "register_timelapse\n",
      "remove_labels_on_edges\n",
      "remove_noise_edge_preserving\n",
      "remove_small_labels\n",
      "reshape_array\n",
      "return_hello_world\n",
      "rgb_to_grey_image_transform\n",
      "roi_imagej_to_ezomero\n",
      "rotate_image_by_90_degrees\n",
      "save_image_with_voxel_size\n",
      "scale_image_affine_transform\n",
      "select_coexpressing_cells\n",
      "stack_and_merge\n",
      "subsample_image\n",
      "subtract_background_tophat\n",
      "sum_images\n",
      "sum_intensity_projection\n",
      "tiled_image_processing\n",
      "translate_3d_image_along_vector\n",
      "transpose_image_axes\n",
      "t_test\n",
      "workflow_batch_process_folder_count_labels\n",
      "workflow_batch_process_folder_measure_intensity\n",
      "workflow_segmentation_counting\n",
      "workflow_segmentation_measurement_summary\n",
      "workflow_segment_measure_umap\n",
      "workflow_watershed_segmentation_correction_measurement\n"
     ]
    }
   ],
   "source": [
    "filename = source_notebook_directory + \"readme.md\"\n",
    "n = len(list_of_cases)\n",
    "header = f\"\"\"\n",
    "# List of use-cases\n",
    "This list of {n} use-cases is auto-generated. Do not modify this file.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(header)\n",
    "    for test_case in list_of_cases:\n",
    "\n",
    "        name = test_case[\"entry_point\"]\n",
    "        task = test_case[\"task_id\"].replace(\"../test_cases/\", \"\")\n",
    "\n",
    "        print(name)\n",
    "        description = test_case[\"prompt\"].split('\"\"\"')[-2]\n",
    "        \n",
    "        file.write(f\"\"\"\n",
    "* [{name}]({task}): {description}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b968bacc-ae6a-48bb-85cc-3fac49ef1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abe637-ecc4-4f23-b9c4-ad62c5601d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
